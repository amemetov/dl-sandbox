{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(train_dataset, train_labels,\n",
    "        graph, tf_train_dataset, tf_train_labels, loss, optimizer, \n",
    "        train_prediction, valid_prediction, test_prediction, \n",
    "        num_steps, batch_size):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        #if (step % 500 == 0):\n",
    "        if (step % (num_steps / 10) == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildLogitGraph(batch_size, L2):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # Variables.\n",
    "        weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "        \n",
    "        \n",
    "        weigths_reg = tf.nn.l2_loss(weights)\n",
    "        loss += L2 * weigths_reg\n",
    "\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "        \n",
    "    return (graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.574633\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 13.0%\n",
      "Minibatch loss at step 300: 3.222669\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 600: 2.211491\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 900: 1.542102\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1200: 1.481816\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 1.177761\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1800: 0.948772\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2100: 0.996795\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2400: 0.900701\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2700: 0.773153\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 3000: 0.766949\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_steps = 3001\n",
    "L2 = 0.001\n",
    "\n",
    "graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction = buildLogitGraph(batch_size, L2)\n",
    "sgd(train_dataset, train_labels, graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildOneHiddenNNGraph(batch_size, hidden_nodes, L2):\n",
    "    one_hidden_layer_graph = tf.Graph()\n",
    "    with one_hidden_layer_graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # Variables.\n",
    "\n",
    "        # First Layer\n",
    "        weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "        biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "        # Activation of First Layer\n",
    "        a1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "\n",
    "        # RELu\n",
    "        relu = tf.nn.relu(a1)\n",
    "        \n",
    "        # Hidden Layer\n",
    "        weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        logits = tf.matmul(relu, weights2) + biases2\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "        \n",
    "        # L2 Regularization\n",
    "        weigths1_reg = tf.nn.l2_loss(weights1)\n",
    "        weights2_reg = tf.nn.l2_loss(weights2)\n",
    "        \n",
    "        loss += L2 * weigths1_reg\n",
    "        loss += L2 * weights2_reg\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Validation\n",
    "        valid_a1 = tf.matmul(tf_valid_dataset, weights1) + biases1\n",
    "        valid_relu = tf.nn.relu(valid_a1)\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, weights2) + biases2)\n",
    "\n",
    "        # Test\n",
    "        test_a1 = tf.matmul(tf_test_dataset, weights1) + biases1\n",
    "        test_relu = tf.nn.relu(test_a1)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights2) + biases2)\n",
    "    \n",
    "    return (one_hidden_layer_graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 606.954163\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 300: 234.043777\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 600: 173.837418\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 900: 126.945679\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1200: 92.697968\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1500: 68.705315\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1800: 50.409191\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2100: 37.588554\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2400: 27.873928\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2700: 20.693211\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3000: 15.457846\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 92.9%\n"
     ]
    }
   ],
   "source": [
    "hidden_nodes = 1024\n",
    "batch_size = 128\n",
    "num_steps = 3001\n",
    "L2 = 0.001\n",
    "\n",
    "graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction = buildOneHiddenNNGraph(batch_size, hidden_nodes, L2)\n",
    "sgd(train_dataset, train_labels, graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 298.214325\n",
      "Minibatch accuracy: 12.0%\n",
      "Validation accuracy: 24.2%\n",
      "Minibatch loss at step 10: 96.382179\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 20: 11.613085\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 30: 16.331694\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 40: 4.388021\n",
      "Minibatch accuracy: 96.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 50: 0.720822\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 90: 0.000051\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.6%\n",
      "Test accuracy: 84.4%\n"
     ]
    }
   ],
   "source": [
    "small_train_num = 1000\n",
    "small_train_dataset = train_dataset[:small_train_num]\n",
    "small_train_labels = train_labels[:small_train_num]\n",
    "\n",
    "hidden_nodes = 1024\n",
    "batch_size = 100\n",
    "num_steps = 101\n",
    "L2 = 0\n",
    "\n",
    "graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction = buildOneHiddenNNGraph(batch_size, hidden_nodes, L2)\n",
    "sgd(small_train_dataset, small_train_labels, graph, tf_train_dataset, tf_train_labels, loss, optimizer, train_prediction, valid_prediction, test_prediction, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_with_dropout(train_dataset, train_labels,\n",
    "                     graph, tf_train_dataset, tf_train_labels, loss, optimizer, \n",
    "        train_prediction, valid_prediction, test_prediction, num_steps, batch_size, \n",
    "        tf_dropout, dropout_prob):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_dropout : dropout_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 500 == 0):\n",
    "            if (step % (num_steps / 10) == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\n",
    "                # Don't dropout\n",
    "                valid_feed_dict = {tf_dropout : 1.0}\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(feed_dict=valid_feed_dict), valid_labels))\n",
    "\n",
    "        # Don't dropout\n",
    "        test_feed_dict = {tf_dropout : 1.0}\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(feed_dict=test_feed_dict), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildOneHiddenNNGraph_with_dropout(batch_size, hidden_nodes, L2):\n",
    "    one_hidden_layer_graph = tf.Graph()\n",
    "    with one_hidden_layer_graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # Variables.\n",
    "\n",
    "        # First Layer\n",
    "        weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "        biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "        # Activation of First Layer\n",
    "        a1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "\n",
    "        # RELu\n",
    "        relu = tf.nn.relu(a1)\n",
    "        \n",
    "        # Dropout\n",
    "        # Using placeholder allow us to use Dropout only during Training process (not Evaluating)\n",
    "        tf_dropout = tf.placeholder(tf.float32)\n",
    "        dropout = tf.nn.dropout(relu, tf_dropout)\n",
    "\n",
    "        # Hidden Layer\n",
    "        weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "        # Training computation.\n",
    "        logits = tf.matmul(dropout, weights2) + biases2\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "        \n",
    "        # L2 Regularization\n",
    "        weigths1_reg = tf.nn.l2_loss(weights1)\n",
    "        weights2_reg = tf.nn.l2_loss(weights2)\n",
    "        \n",
    "        loss += L2 * weigths1_reg\n",
    "        loss += L2 * weights2_reg\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Validation\n",
    "        valid_a1 = tf.matmul(tf_valid_dataset, weights1) + biases1\n",
    "        valid_relu = tf.nn.relu(valid_a1)\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(valid_relu, weights2) + biases2)\n",
    "\n",
    "        # Test\n",
    "        test_a1 = tf.matmul(tf_test_dataset, weights1) + biases1\n",
    "        test_relu = tf.nn.relu(test_a1)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights2) + biases2)\n",
    "    \n",
    "    return (one_hidden_layer_graph, tf_train_dataset, tf_train_labels, tf_dropout,\n",
    "            loss, optimizer, train_prediction, valid_prediction, test_prediction)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 788.572876\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 23.9%\n",
      "Minibatch loss at step 300: 240.957367\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 600: 186.304520\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 900: 129.192978\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1200: 94.336266\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1500: 69.613640\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1800: 51.091213\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2100: 37.907959\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2400: 27.931246\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2700: 20.670872\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3000: 15.523764\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 91.9%\n"
     ]
    }
   ],
   "source": [
    "hidden_nodes = 1024\n",
    "batches_num = 100\n",
    "batch_size = 128#train_dataset.shape[0] / batches_num\n",
    "num_steps = 3001\n",
    "L2 = 0.001\n",
    "dropout = 0.5\n",
    "\n",
    "graph, tf_train_dataset, tf_train_labels, tf_dropout, loss, optimizer, train_prediction, valid_prediction, test_prediction = buildOneHiddenNNGraph_with_dropout(batch_size, hidden_nodes, L2)\n",
    "\n",
    "sgd_with_dropout(train_dataset, train_labels, graph, tf_train_dataset, tf_train_labels, loss, optimizer, \n",
    "                 train_prediction, valid_prediction, test_prediction, num_steps, batch_size,\n",
    "                tf_dropout, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mln import MultiLayerNet, Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = image_size * image_size\n",
    "hidden_dims = [1024, 300, 50]\n",
    "num_classes = num_labels\n",
    "batches_num = 100\n",
    "batch_size = train_dataset.shape[0] / batches_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Start NN l2 0.000000e+00 weight_scale 1.000000e-04 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 5.631177\n",
      "Minibatch train accuracy: 11.2%\n",
      "Validation accuracy: 11.1%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 2.035229\n",
      "Minibatch train accuracy: 57.3%\n",
      "Validation accuracy: 58.7%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.517480\n",
      "Minibatch train accuracy: 75.8%\n",
      "Validation accuracy: 75.3%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 1.325798\n",
      "Minibatch train accuracy: 77.9%\n",
      "Validation accuracy: 78.6%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 1.159116\n",
      "Minibatch train accuracy: 79.5%\n",
      "Validation accuracy: 80.0%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 1.021176\n",
      "Minibatch train accuracy: 80.5%\n",
      "Validation accuracy: 80.7%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.944296\n",
      "Minibatch train accuracy: 81.6%\n",
      "Validation accuracy: 81.4%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 0.864458\n",
      "Minibatch train accuracy: 83.2%\n",
      "Validation accuracy: 82.0%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.825579\n",
      "Minibatch train accuracy: 83.4%\n",
      "Validation accuracy: 82.2%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.801730\n",
      "Minibatch train accuracy: 82.2%\n",
      "Validation accuracy: 82.5%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.763606\n",
      "Minibatch train accuracy: 84.2%\n",
      "Validation accuracy: 82.9%\n",
      "-----------------------------------------\n",
      "Start NN l2 0.000000e+00 weight_scale 1.000000e-03 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 5.482236\n",
      "Minibatch train accuracy: 10.1%\n",
      "Validation accuracy: 9.1%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 2.046767\n",
      "Minibatch train accuracy: 63.3%\n",
      "Validation accuracy: 62.4%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.542298\n",
      "Minibatch train accuracy: 75.4%\n",
      "Validation accuracy: 75.2%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 1.299547\n",
      "Minibatch train accuracy: 77.9%\n",
      "Validation accuracy: 78.6%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 1.149611\n",
      "Minibatch train accuracy: 79.4%\n",
      "Validation accuracy: 79.9%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 1.028767\n",
      "Minibatch train accuracy: 80.7%\n",
      "Validation accuracy: 80.9%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.942524\n",
      "Minibatch train accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 0.858491\n",
      "Minibatch train accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.791811\n",
      "Minibatch train accuracy: 83.1%\n",
      "Validation accuracy: 82.6%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.799043\n",
      "Minibatch train accuracy: 82.9%\n",
      "Validation accuracy: 82.8%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.743494\n",
      "Minibatch train accuracy: 84.4%\n",
      "Validation accuracy: 83.1%\n",
      "-----------------------------------------\n",
      "Start NN l2 0.000000e+00 weight_scale 1.000000e-02 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 5.589620\n",
      "Minibatch train accuracy: 10.2%\n",
      "Validation accuracy: 10.3%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 1.964673\n",
      "Minibatch train accuracy: 68.8%\n",
      "Validation accuracy: 68.3%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.547062\n",
      "Minibatch train accuracy: 77.1%\n",
      "Validation accuracy: 76.5%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 1.247924\n",
      "Minibatch train accuracy: 77.4%\n",
      "Validation accuracy: 79.1%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 1.078849\n",
      "Minibatch train accuracy: 80.2%\n",
      "Validation accuracy: 80.6%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 0.977370\n",
      "Minibatch train accuracy: 80.5%\n",
      "Validation accuracy: 81.1%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.927525\n",
      "Minibatch train accuracy: 81.4%\n",
      "Validation accuracy: 81.6%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 0.850662\n",
      "Minibatch train accuracy: 83.8%\n",
      "Validation accuracy: 82.3%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.811628\n",
      "Minibatch train accuracy: 82.9%\n",
      "Validation accuracy: 82.7%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.784248\n",
      "Minibatch train accuracy: 82.7%\n",
      "Validation accuracy: 82.8%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.730097\n",
      "Minibatch train accuracy: 84.1%\n",
      "Validation accuracy: 83.2%\n",
      "-----------------------------------------\n",
      "Start NN l2 0.000000e+00 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 4.900605\n",
      "Minibatch train accuracy: 7.8%\n",
      "Validation accuracy: 8.1%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 1.926298\n",
      "Minibatch train accuracy: 68.8%\n",
      "Validation accuracy: 68.4%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.446284\n",
      "Minibatch train accuracy: 78.0%\n",
      "Validation accuracy: 77.0%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 1.172976\n",
      "Minibatch train accuracy: 79.0%\n",
      "Validation accuracy: 79.6%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 1.036507\n",
      "Minibatch train accuracy: 80.2%\n",
      "Validation accuracy: 80.9%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 0.925995\n",
      "Minibatch train accuracy: 81.3%\n",
      "Validation accuracy: 81.5%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.897173\n",
      "Minibatch train accuracy: 81.6%\n",
      "Validation accuracy: 82.2%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 0.826298\n",
      "Minibatch train accuracy: 83.7%\n",
      "Validation accuracy: 82.5%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.778380\n",
      "Minibatch train accuracy: 83.5%\n",
      "Validation accuracy: 82.7%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.777637\n",
      "Minibatch train accuracy: 83.2%\n",
      "Validation accuracy: 83.0%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.751846\n",
      "Minibatch train accuracy: 84.4%\n",
      "Validation accuracy: 83.2%\n",
      "-----------------------------------------\n",
      "Start NN l2 0.000000e+00 weight_scale 1.000000e+00 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 5.338172\n",
      "Minibatch train accuracy: 10.1%\n",
      "Validation accuracy: 10.8%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 1.962505\n",
      "Minibatch train accuracy: 66.4%\n",
      "Validation accuracy: 67.1%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.507942\n",
      "Minibatch train accuracy: 75.5%\n",
      "Validation accuracy: 75.4%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 1.320785\n",
      "Minibatch train accuracy: 77.4%\n",
      "Validation accuracy: 78.5%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 1.072467\n",
      "Minibatch train accuracy: 79.1%\n",
      "Validation accuracy: 79.9%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 0.975520\n",
      "Minibatch train accuracy: 81.1%\n",
      "Validation accuracy: 80.9%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.938534\n",
      "Minibatch train accuracy: 80.4%\n",
      "Validation accuracy: 81.6%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 0.845808\n",
      "Minibatch train accuracy: 83.2%\n",
      "Validation accuracy: 81.9%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.802609\n",
      "Minibatch train accuracy: 83.5%\n",
      "Validation accuracy: 82.3%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.804477\n",
      "Minibatch train accuracy: 83.0%\n",
      "Validation accuracy: 82.7%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.731272\n",
      "Minibatch train accuracy: 84.6%\n",
      "Validation accuracy: 83.0%\n",
      "*************** Finished ***************\n",
      "Best Valid Accuracy: 83.2% on L2: 0.000000e+00, WeightScale: 1.000000e-01 -----------------\n",
      "*************** Test accuracy: 90.0% *****************\n"
     ]
    }
   ],
   "source": [
    "#Find best weight scale\n",
    "\n",
    "num_epochs = 10\n",
    "dropout_prob = 0.5\n",
    "\n",
    "l2 = 0\n",
    "weight_scales = np.logspace(-4, 0, num=5)\n",
    "lr_start = 1e-4\n",
    "lr_decay_steps = 100000\n",
    "lr_decay_rate = 0.96\n",
    "\n",
    "best_valid_loss = 0\n",
    "best_valid_accuracy = 0\n",
    "best_valid_weights = None\n",
    "best_valid_biases = None\n",
    "best_valid_ws = 0\n",
    "\n",
    "best_valid_loss_history = []\n",
    "best_valid_accs_history = []\n",
    "\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "    print ('-----------------------------------------')\n",
    "    print ('Start NN l2 %e weight_scale %e ' % (l2, weight_scale))\n",
    "\n",
    "    mln = MultiLayerNet(input_dim, hidden_dims, num_classes, l2)\n",
    "    solver = Solver(mln, train_dataset, train_labels, valid_dataset, valid_labels, \n",
    "                    batch_size, num_epochs, dropout_prob, \n",
    "                   lr_start, lr_decay_steps, lr_decay_rate)\n",
    "    (valid_loss, valid_accuracy, valid_weights, valid_biases) = solver.train()\n",
    "\n",
    "    best_valid_loss_history.append(valid_loss)\n",
    "    best_valid_accs_history.append(valid_accuracy)\n",
    "\n",
    "    if valid_accuracy > best_valid_accuracy:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_valid_accuracy = valid_accuracy\n",
    "        best_valid_weights = valid_weights\n",
    "        best_valid_biases = valid_biases\n",
    "        best_valid_ws = weight_scale\n",
    "\n",
    "        \n",
    "print(\"*************** Finished ***************\")\n",
    "print(\"Best Valid Accuracy: %.1f%% on L2: %e, WeightScale: %e -----------------\" % (best_valid_accuracy*100, l2, best_valid_ws))\n",
    "\n",
    "test_accuracy_model = MultiLayerNet(input_dim, hidden_dims, num_classes, l2, best_valid_ws, best_valid_weights, best_valid_biases)\n",
    "with tf.Session(graph=test_accuracy_model.graph) as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    test_accuracy = test_accuracy_model.accuracy.eval(feed_dict={test_accuracy_model.X:test_dataset, \n",
    "                                                             test_accuracy_model.Y:test_labels, \n",
    "                                                             test_accuracy_model.dropout: 1.0})\n",
    "print(\"*************** Test accuracy: %.1f%% *****************\" % (test_accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAJcCAYAAABjQXlgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF3xmZmZmLe5/OB3+n2Vp+zkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3594d61d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best valid accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best valid accuracy')\n",
    "plt.semilogx(weight_scales, best_valid_accs_history, '-o', label='baseline')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Loss')\n",
    "plt.semilogx(weight_scales, best_valid_loss_history, '-o')\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Start NN l2 1.000000e-03 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 9.510757\n",
      "Minibatch train accuracy: 22.0%\n",
      "Validation accuracy: 22.4%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 3.043007\n",
      "Minibatch train accuracy: 83.7%\n",
      "Validation accuracy: 83.9%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.560463\n",
      "Minibatch train accuracy: 87.2%\n",
      "Validation accuracy: 85.3%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 0.977196\n",
      "Minibatch train accuracy: 86.9%\n",
      "Validation accuracy: 86.1%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 0.782926\n",
      "Minibatch train accuracy: 87.2%\n",
      "Validation accuracy: 86.7%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 0.742308\n",
      "Minibatch train accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.645575\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 87.3%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 0.637597\n",
      "Minibatch train accuracy: 87.9%\n",
      "Validation accuracy: 87.5%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.647774\n",
      "Minibatch train accuracy: 88.4%\n",
      "Validation accuracy: 87.8%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.616164\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 88.1%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.593818\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "-----------------------------------------\n",
      "Start NN l2 1.000000e-02 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 48.725262\n",
      "Minibatch train accuracy: 25.7%\n",
      "Validation accuracy: 25.1%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 2.824906\n",
      "Minibatch train accuracy: 83.5%\n",
      "Validation accuracy: 83.2%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 1.023027\n",
      "Minibatch train accuracy: 84.6%\n",
      "Validation accuracy: 83.5%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 0.964580\n",
      "Minibatch train accuracy: 84.3%\n",
      "Validation accuracy: 83.6%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 0.969056\n",
      "Minibatch train accuracy: 84.3%\n",
      "Validation accuracy: 83.9%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 0.959428\n",
      "Minibatch train accuracy: 84.1%\n",
      "Validation accuracy: 83.6%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 0.924547\n",
      "Minibatch train accuracy: 85.9%\n",
      "Validation accuracy: 84.0%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 1.015746\n",
      "Minibatch train accuracy: 83.8%\n",
      "Validation accuracy: 83.9%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 0.988299\n",
      "Minibatch train accuracy: 84.0%\n",
      "Validation accuracy: 83.8%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 0.954591\n",
      "Minibatch train accuracy: 84.6%\n",
      "Validation accuracy: 83.8%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 0.912844\n",
      "Minibatch train accuracy: 85.4%\n",
      "Validation accuracy: 83.5%\n",
      "-----------------------------------------\n",
      "Start NN l2 1.000000e-01 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 440.216522\n",
      "Minibatch train accuracy: 9.9%\n",
      "Validation accuracy: 8.6%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 9.415278\n",
      "Minibatch train accuracy: 73.8%\n",
      "Validation accuracy: 75.4%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 2.070780\n",
      "Minibatch train accuracy: 74.9%\n",
      "Validation accuracy: 73.2%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 2.006679\n",
      "Minibatch train accuracy: 74.5%\n",
      "Validation accuracy: 73.2%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 2.015615\n",
      "Minibatch train accuracy: 73.1%\n",
      "Validation accuracy: 72.2%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 2.017904\n",
      "Minibatch train accuracy: 73.9%\n",
      "Validation accuracy: 73.5%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 1.996940\n",
      "Minibatch train accuracy: 75.5%\n",
      "Validation accuracy: 72.6%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 2.036279\n",
      "Minibatch train accuracy: 73.0%\n",
      "Validation accuracy: 72.7%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 2.022682\n",
      "Minibatch train accuracy: 74.2%\n",
      "Validation accuracy: 72.3%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 2.008157\n",
      "Minibatch train accuracy: 73.3%\n",
      "Validation accuracy: 72.2%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 1.973969\n",
      "Minibatch train accuracy: 74.1%\n",
      "Validation accuracy: 72.9%\n",
      "-----------------------------------------\n",
      "Start NN l2 1.000000e+00 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 4354.669922\n",
      "Minibatch train accuracy: 9.9%\n",
      "Validation accuracy: 10.9%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 74.029678\n",
      "Minibatch train accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 2.665495\n",
      "Minibatch train accuracy: 8.5%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 2.303036\n",
      "Minibatch train accuracy: 9.5%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 2.302517\n",
      "Minibatch train accuracy: 9.2%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 2.302398\n",
      "Minibatch train accuracy: 10.4%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 2.302627\n",
      "Minibatch train accuracy: 9.6%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 2.302738\n",
      "Minibatch train accuracy: 9.4%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 2.302489\n",
      "Minibatch train accuracy: 9.3%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 2.302622\n",
      "Minibatch train accuracy: 10.4%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 2.302567\n",
      "Minibatch train accuracy: 10.3%\n",
      "Validation accuracy: 10.0%\n",
      "-----------------------------------------\n",
      "Start NN l2 1.000000e+01 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 10 ----------------------\n",
      "Minibatch loss: 43517.695312\n",
      "Minibatch train accuracy: 10.6%\n",
      "Validation accuracy: 11.6%\n",
      "------------------- Epoch 1 of 10 ----------------------\n",
      "Minibatch loss: 718.066711\n",
      "Minibatch train accuracy: 11.0%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 2 of 10 ----------------------\n",
      "Minibatch loss: 5.869129\n",
      "Minibatch train accuracy: 9.8%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 3 of 10 ----------------------\n",
      "Minibatch loss: 2.306333\n",
      "Minibatch train accuracy: 11.4%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 4 of 10 ----------------------\n",
      "Minibatch loss: 2.302653\n",
      "Minibatch train accuracy: 9.2%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 5 of 10 ----------------------\n",
      "Minibatch loss: 2.302470\n",
      "Minibatch train accuracy: 10.4%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 6 of 10 ----------------------\n",
      "Minibatch loss: 2.302590\n",
      "Minibatch train accuracy: 11.5%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 7 of 10 ----------------------\n",
      "Minibatch loss: 2.302708\n",
      "Minibatch train accuracy: 9.2%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 8 of 10 ----------------------\n",
      "Minibatch loss: 2.302485\n",
      "Minibatch train accuracy: 11.1%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 9 of 10 ----------------------\n",
      "Minibatch loss: 2.302632\n",
      "Minibatch train accuracy: 9.7%\n",
      "Validation accuracy: 10.0%\n",
      "------------------- Epoch 10 of 10 ----------------------\n",
      "Minibatch loss: 2.302584\n",
      "Minibatch train accuracy: 10.4%\n",
      "Validation accuracy: 10.0%\n",
      "*************** Finished ***************\n",
      "Best Valid Accuracy: 88.2% on L2: 1.000000e-03, WeightScale: 1.000000e-01 -----------------\n",
      "*************** Test accuracy: 93.9% *****************\n"
     ]
    }
   ],
   "source": [
    "#Find best L2 value\n",
    "\n",
    "hidden_dims = [1024, 300, 50]\n",
    "#hidden_dims = [4096, 2048, 1024]\n",
    "batches_num = 100\n",
    "batch_size = 1024#train_dataset.shape[0] / batches_num\n",
    "\n",
    "num_epochs = 10\n",
    "dropout_prob = 0.5\n",
    "\n",
    "l2_strengths = np.logspace(-5, -4, num=2)\n",
    "#l2_strengths = [0.001]\n",
    "lr_start = 1e-3\n",
    "lr_decay_steps = 100000\n",
    "lr_decay_rate = 0.96\n",
    "\n",
    "best_valid_l2_loss = 0\n",
    "best_valid_l2_accuracy = 0\n",
    "best_valid_l2_weights = None\n",
    "best_valid_l2_biases = None\n",
    "best_valid_l2_val = 0\n",
    "\n",
    "best_valid_l2_loss_history = []\n",
    "best_valid_l2_accs_history = []\n",
    "\n",
    "for l2 in l2_strengths:\n",
    "    \n",
    "    print ('-----------------------------------------')\n",
    "    print ('Start NN l2 %e weight_scale %e ' % (l2, best_valid_ws))\n",
    "\n",
    "    mln = MultiLayerNet(input_dim, hidden_dims, num_classes, l2)\n",
    "    solver = Solver(mln, train_dataset, train_labels, valid_dataset, valid_labels, \n",
    "                    batch_size, num_epochs, dropout_prob, \n",
    "                    lr_start, lr_decay_steps, lr_decay_rate)\n",
    "    (valid_loss, valid_accuracy, valid_weights, valid_biases) = solver.train()\n",
    "\n",
    "    best_valid_l2_loss_history.append(valid_loss)\n",
    "    best_valid_l2_accs_history.append(valid_accuracy)\n",
    "\n",
    "    if valid_accuracy > best_valid_l2_accuracy:\n",
    "        best_valid_l2_loss = valid_loss\n",
    "        best_valid_l2_accuracy = valid_accuracy\n",
    "        best_valid_l2_weights = valid_weights\n",
    "        best_valid_l2_biases = valid_biases\n",
    "        best_valid_l2_val = l2\n",
    "\n",
    "        \n",
    "print(\"*************** Finished ***************\")\n",
    "print(\"Best Valid Accuracy: %.1f%% on L2: %e, WeightScale: %e -----------------\" % (best_valid_l2_accuracy*100, \n",
    "                                                                                    best_valid_l2_val, best_valid_ws))\n",
    "\n",
    "test_accuracy_model = MultiLayerNet(input_dim, hidden_dims, num_classes,\n",
    "                                    best_valid_l2_val, best_valid_ws, best_valid_l2_weights, best_valid_l2_biases)\n",
    "with tf.Session(graph=test_accuracy_model.graph) as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    test_accuracy = test_accuracy_model.accuracy.eval(feed_dict={test_accuracy_model.X:test_dataset, \n",
    "                                                             test_accuracy_model.Y:test_labels, \n",
    "                                                             test_accuracy_model.dropout: 1.0})\n",
    "print(\"*************** Test accuracy: %.1f%% *****************\" % (test_accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAJcCAYAAABqqtkaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3589abad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best valid accuracy vs L2')\n",
    "plt.xlabel('L2')\n",
    "plt.ylabel('Best valid accuracy')\n",
    "plt.semilogx(l2_strengths, best_valid_l2_accs_history, '-o')\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Loss vs L2')\n",
    "plt.xlabel('L2')\n",
    "plt.ylabel('Loss')\n",
    "plt.semilogx(l2_strengths, best_valid_l2_loss_history, '-o')\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Start NN l2 1.000000e-03 weight_scale 1.000000e-01 \n",
      "Initialized\n",
      "------------------- Epoch 0 of 500 ----------------------\n",
      "Minibatch loss: 9.958504\n",
      "Minibatch train accuracy: 23.0%\n",
      "Validation accuracy: 22.5%\n",
      "------------------- Epoch 1 of 500 ----------------------\n",
      "Minibatch loss: 3.064637\n",
      "Minibatch train accuracy: 83.6%\n",
      "Validation accuracy: 84.0%\n",
      "------------------- Epoch 2 of 500 ----------------------\n",
      "Minibatch loss: 1.609797\n",
      "Minibatch train accuracy: 86.0%\n",
      "Validation accuracy: 85.1%\n",
      "------------------- Epoch 3 of 500 ----------------------\n",
      "Minibatch loss: 1.012100\n",
      "Minibatch train accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "------------------- Epoch 4 of 500 ----------------------\n",
      "Minibatch loss: 0.794315\n",
      "Minibatch train accuracy: 88.1%\n",
      "Validation accuracy: 86.7%\n",
      "------------------- Epoch 5 of 500 ----------------------\n",
      "Minibatch loss: 0.715869\n",
      "Minibatch train accuracy: 88.5%\n",
      "Validation accuracy: 87.1%\n",
      "------------------- Epoch 6 of 500 ----------------------\n",
      "Minibatch loss: 0.646117\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 87.6%\n",
      "------------------- Epoch 7 of 500 ----------------------\n",
      "Minibatch loss: 0.679566\n",
      "Minibatch train accuracy: 87.6%\n",
      "Validation accuracy: 87.7%\n",
      "------------------- Epoch 8 of 500 ----------------------\n",
      "Minibatch loss: 0.656068\n",
      "Minibatch train accuracy: 88.5%\n",
      "Validation accuracy: 87.9%\n",
      "------------------- Epoch 9 of 500 ----------------------\n",
      "Minibatch loss: 0.619675\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 87.9%\n",
      "------------------- Epoch 10 of 500 ----------------------\n",
      "Minibatch loss: 0.570966\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.2%\n",
      "------------------- Epoch 11 of 500 ----------------------\n",
      "Minibatch loss: 0.653357\n",
      "Minibatch train accuracy: 87.7%\n",
      "Validation accuracy: 88.0%\n",
      "------------------- Epoch 12 of 500 ----------------------\n",
      "Minibatch loss: 0.684990\n",
      "Minibatch train accuracy: 88.1%\n",
      "Validation accuracy: 88.1%\n",
      "------------------- Epoch 13 of 500 ----------------------\n",
      "Minibatch loss: 0.613647\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 87.9%\n",
      "------------------- Epoch 14 of 500 ----------------------\n",
      "Minibatch loss: 0.578669\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 87.9%\n",
      "------------------- Epoch 15 of 500 ----------------------\n",
      "Minibatch loss: 0.652083\n",
      "Minibatch train accuracy: 88.9%\n",
      "Validation accuracy: 88.0%\n",
      "------------------- Epoch 16 of 500 ----------------------\n",
      "Minibatch loss: 0.628135\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 88.2%\n",
      "------------------- Epoch 17 of 500 ----------------------\n",
      "Minibatch loss: 0.573721\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 88.3%\n",
      "------------------- Epoch 18 of 500 ----------------------\n",
      "Minibatch loss: 0.599879\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 88.3%\n",
      "------------------- Epoch 19 of 500 ----------------------\n",
      "Minibatch loss: 0.600555\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 88.3%\n",
      "------------------- Epoch 20 of 500 ----------------------\n",
      "Minibatch loss: 0.624482\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 88.5%\n",
      "------------------- Epoch 21 of 500 ----------------------\n",
      "Minibatch loss: 0.596196\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.2%\n",
      "------------------- Epoch 22 of 500 ----------------------\n",
      "Minibatch loss: 0.573424\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.4%\n",
      "------------------- Epoch 23 of 500 ----------------------\n",
      "Minibatch loss: 0.604078\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 88.2%\n",
      "------------------- Epoch 24 of 500 ----------------------\n",
      "Minibatch loss: 0.604471\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 88.1%\n",
      "------------------- Epoch 25 of 500 ----------------------\n",
      "Minibatch loss: 0.602984\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.3%\n",
      "------------------- Epoch 26 of 500 ----------------------\n",
      "Minibatch loss: 0.552420\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 88.2%\n",
      "------------------- Epoch 27 of 500 ----------------------\n",
      "Minibatch loss: 0.578001\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 88.5%\n",
      "------------------- Epoch 28 of 500 ----------------------\n",
      "Minibatch loss: 0.573069\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 88.3%\n",
      "------------------- Epoch 29 of 500 ----------------------\n",
      "Minibatch loss: 0.634707\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 88.5%\n",
      "------------------- Epoch 30 of 500 ----------------------\n",
      "Minibatch loss: 0.591396\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 31 of 500 ----------------------\n",
      "Minibatch loss: 0.620739\n",
      "Minibatch train accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "------------------- Epoch 32 of 500 ----------------------\n",
      "Minibatch loss: 0.569106\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 33 of 500 ----------------------\n",
      "Minibatch loss: 0.589054\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 34 of 500 ----------------------\n",
      "Minibatch loss: 0.617294\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 88.5%\n",
      "------------------- Epoch 35 of 500 ----------------------\n",
      "Minibatch loss: 0.624873\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 36 of 500 ----------------------\n",
      "Minibatch loss: 0.607272\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 37 of 500 ----------------------\n",
      "Minibatch loss: 0.631997\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 38 of 500 ----------------------\n",
      "Minibatch loss: 0.555976\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 39 of 500 ----------------------\n",
      "Minibatch loss: 0.511102\n",
      "Minibatch train accuracy: 91.7%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 40 of 500 ----------------------\n",
      "Minibatch loss: 0.563144\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 41 of 500 ----------------------\n",
      "Minibatch loss: 0.582402\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 42 of 500 ----------------------\n",
      "Minibatch loss: 0.595550\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 43 of 500 ----------------------\n",
      "Minibatch loss: 0.572189\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 44 of 500 ----------------------\n",
      "Minibatch loss: 0.565935\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 45 of 500 ----------------------\n",
      "Minibatch loss: 0.544022\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 46 of 500 ----------------------\n",
      "Minibatch loss: 0.537181\n",
      "Minibatch train accuracy: 91.6%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 47 of 500 ----------------------\n",
      "Minibatch loss: 0.578510\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 48 of 500 ----------------------\n",
      "Minibatch loss: 0.523700\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 49 of 500 ----------------------\n",
      "Minibatch loss: 0.514155\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 50 of 500 ----------------------\n",
      "Minibatch loss: 0.558263\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 51 of 500 ----------------------\n",
      "Minibatch loss: 0.562303\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 52 of 500 ----------------------\n",
      "Minibatch loss: 0.610665\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 53 of 500 ----------------------\n",
      "Minibatch loss: 0.573955\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 54 of 500 ----------------------\n",
      "Minibatch loss: 0.512234\n",
      "Minibatch train accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 55 of 500 ----------------------\n",
      "Minibatch loss: 0.535192\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 56 of 500 ----------------------\n",
      "Minibatch loss: 0.548197\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 57 of 500 ----------------------\n",
      "Minibatch loss: 0.563768\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 58 of 500 ----------------------\n",
      "Minibatch loss: 0.605053\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 59 of 500 ----------------------\n",
      "Minibatch loss: 0.569766\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 60 of 500 ----------------------\n",
      "Minibatch loss: 0.592705\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 61 of 500 ----------------------\n",
      "Minibatch loss: 0.586644\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 62 of 500 ----------------------\n",
      "Minibatch loss: 0.628005\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 63 of 500 ----------------------\n",
      "Minibatch loss: 0.641246\n",
      "Minibatch train accuracy: 88.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 64 of 500 ----------------------\n",
      "Minibatch loss: 0.595310\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 65 of 500 ----------------------\n",
      "Minibatch loss: 0.498710\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 66 of 500 ----------------------\n",
      "Minibatch loss: 0.578502\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 67 of 500 ----------------------\n",
      "Minibatch loss: 0.575153\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 68 of 500 ----------------------\n",
      "Minibatch loss: 0.566836\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 69 of 500 ----------------------\n",
      "Minibatch loss: 0.616124\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 70 of 500 ----------------------\n",
      "Minibatch loss: 0.566823\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 71 of 500 ----------------------\n",
      "Minibatch loss: 0.559196\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 72 of 500 ----------------------\n",
      "Minibatch loss: 0.585901\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 73 of 500 ----------------------\n",
      "Minibatch loss: 0.613432\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 74 of 500 ----------------------\n",
      "Minibatch loss: 0.595398\n",
      "Minibatch train accuracy: 88.3%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 75 of 500 ----------------------\n",
      "Minibatch loss: 0.564511\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 76 of 500 ----------------------\n",
      "Minibatch loss: 0.515798\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 77 of 500 ----------------------\n",
      "Minibatch loss: 0.629899\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 78 of 500 ----------------------\n",
      "Minibatch loss: 0.612041\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 79 of 500 ----------------------\n",
      "Minibatch loss: 0.584155\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 80 of 500 ----------------------\n",
      "Minibatch loss: 0.571414\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 81 of 500 ----------------------\n",
      "Minibatch loss: 0.577326\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 82 of 500 ----------------------\n",
      "Minibatch loss: 0.570341\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 83 of 500 ----------------------\n",
      "Minibatch loss: 0.605035\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 84 of 500 ----------------------\n",
      "Minibatch loss: 0.576058\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 85 of 500 ----------------------\n",
      "Minibatch loss: 0.581865\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 86 of 500 ----------------------\n",
      "Minibatch loss: 0.609071\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 87 of 500 ----------------------\n",
      "Minibatch loss: 0.591022\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 88 of 500 ----------------------\n",
      "Minibatch loss: 0.563878\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 89 of 500 ----------------------\n",
      "Minibatch loss: 0.534126\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 90 of 500 ----------------------\n",
      "Minibatch loss: 0.554416\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 91 of 500 ----------------------\n",
      "Minibatch loss: 0.569241\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 92 of 500 ----------------------\n",
      "Minibatch loss: 0.585536\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 93 of 500 ----------------------\n",
      "Minibatch loss: 0.612937\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 94 of 500 ----------------------\n",
      "Minibatch loss: 0.552178\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 95 of 500 ----------------------\n",
      "Minibatch loss: 0.569623\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 96 of 500 ----------------------\n",
      "Minibatch loss: 0.613910\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 97 of 500 ----------------------\n",
      "Minibatch loss: 0.588780\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 98 of 500 ----------------------\n",
      "Minibatch loss: 0.609084\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 99 of 500 ----------------------\n",
      "Minibatch loss: 0.609275\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 100 of 500 ----------------------\n",
      "Minibatch loss: 0.540439\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 101 of 500 ----------------------\n",
      "Minibatch loss: 0.516281\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 102 of 500 ----------------------\n",
      "Minibatch loss: 0.540350\n",
      "Minibatch train accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 103 of 500 ----------------------\n",
      "Minibatch loss: 0.528083\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 104 of 500 ----------------------\n",
      "Minibatch loss: 0.536638\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 105 of 500 ----------------------\n",
      "Minibatch loss: 0.555954\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 106 of 500 ----------------------\n",
      "Minibatch loss: 0.558672\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 107 of 500 ----------------------\n",
      "Minibatch loss: 0.536833\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 108 of 500 ----------------------\n",
      "Minibatch loss: 0.573825\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 109 of 500 ----------------------\n",
      "Minibatch loss: 0.578991\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 110 of 500 ----------------------\n",
      "Minibatch loss: 0.560813\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 111 of 500 ----------------------\n",
      "Minibatch loss: 0.608808\n",
      "Minibatch train accuracy: 88.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 112 of 500 ----------------------\n",
      "Minibatch loss: 0.578771\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 113 of 500 ----------------------\n",
      "Minibatch loss: 0.575108\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 114 of 500 ----------------------\n",
      "Minibatch loss: 0.574164\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 115 of 500 ----------------------\n",
      "Minibatch loss: 0.571107\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 116 of 500 ----------------------\n",
      "Minibatch loss: 0.568750\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 117 of 500 ----------------------\n",
      "Minibatch loss: 0.546122\n",
      "Minibatch train accuracy: 91.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 118 of 500 ----------------------\n",
      "Minibatch loss: 0.628407\n",
      "Minibatch train accuracy: 88.2%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 119 of 500 ----------------------\n",
      "Minibatch loss: 0.583940\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 120 of 500 ----------------------\n",
      "Minibatch loss: 0.549333\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 121 of 500 ----------------------\n",
      "Minibatch loss: 0.530260\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 122 of 500 ----------------------\n",
      "Minibatch loss: 0.578142\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 123 of 500 ----------------------\n",
      "Minibatch loss: 0.583624\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 124 of 500 ----------------------\n",
      "Minibatch loss: 0.536684\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 125 of 500 ----------------------\n",
      "Minibatch loss: 0.545576\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 126 of 500 ----------------------\n",
      "Minibatch loss: 0.559075\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 127 of 500 ----------------------\n",
      "Minibatch loss: 0.538552\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 128 of 500 ----------------------\n",
      "Minibatch loss: 0.572546\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 129 of 500 ----------------------\n",
      "Minibatch loss: 0.605698\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 130 of 500 ----------------------\n",
      "Minibatch loss: 0.548379\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 131 of 500 ----------------------\n",
      "Minibatch loss: 0.524334\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 132 of 500 ----------------------\n",
      "Minibatch loss: 0.537076\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 133 of 500 ----------------------\n",
      "Minibatch loss: 0.577442\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 134 of 500 ----------------------\n",
      "Minibatch loss: 0.518076\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 135 of 500 ----------------------\n",
      "Minibatch loss: 0.610681\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 136 of 500 ----------------------\n",
      "Minibatch loss: 0.581985\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 137 of 500 ----------------------\n",
      "Minibatch loss: 0.567977\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 138 of 500 ----------------------\n",
      "Minibatch loss: 0.544106\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 139 of 500 ----------------------\n",
      "Minibatch loss: 0.554404\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 140 of 500 ----------------------\n",
      "Minibatch loss: 0.569213\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 141 of 500 ----------------------\n",
      "Minibatch loss: 0.560872\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 142 of 500 ----------------------\n",
      "Minibatch loss: 0.549011\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 143 of 500 ----------------------\n",
      "Minibatch loss: 0.590007\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 144 of 500 ----------------------\n",
      "Minibatch loss: 0.548302\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 145 of 500 ----------------------\n",
      "Minibatch loss: 0.590032\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 146 of 500 ----------------------\n",
      "Minibatch loss: 0.556169\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 147 of 500 ----------------------\n",
      "Minibatch loss: 0.561070\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 148 of 500 ----------------------\n",
      "Minibatch loss: 0.539136\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 149 of 500 ----------------------\n",
      "Minibatch loss: 0.526626\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 150 of 500 ----------------------\n",
      "Minibatch loss: 0.553677\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 151 of 500 ----------------------\n",
      "Minibatch loss: 0.562850\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 152 of 500 ----------------------\n",
      "Minibatch loss: 0.527570\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 153 of 500 ----------------------\n",
      "Minibatch loss: 0.553612\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 154 of 500 ----------------------\n",
      "Minibatch loss: 0.548681\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 155 of 500 ----------------------\n",
      "Minibatch loss: 0.652567\n",
      "Minibatch train accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 156 of 500 ----------------------\n",
      "Minibatch loss: 0.539900\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 157 of 500 ----------------------\n",
      "Minibatch loss: 0.561821\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 158 of 500 ----------------------\n",
      "Minibatch loss: 0.577804\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 159 of 500 ----------------------\n",
      "Minibatch loss: 0.598141\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 160 of 500 ----------------------\n",
      "Minibatch loss: 0.521993\n",
      "Minibatch train accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 161 of 500 ----------------------\n",
      "Minibatch loss: 0.559485\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 162 of 500 ----------------------\n",
      "Minibatch loss: 0.543601\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 163 of 500 ----------------------\n",
      "Minibatch loss: 0.609082\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 164 of 500 ----------------------\n",
      "Minibatch loss: 0.612189\n",
      "Minibatch train accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 165 of 500 ----------------------\n",
      "Minibatch loss: 0.590016\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 166 of 500 ----------------------\n",
      "Minibatch loss: 0.545725\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 167 of 500 ----------------------\n",
      "Minibatch loss: 0.555306\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 168 of 500 ----------------------\n",
      "Minibatch loss: 0.566915\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 169 of 500 ----------------------\n",
      "Minibatch loss: 0.614087\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 170 of 500 ----------------------\n",
      "Minibatch loss: 0.585210\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 171 of 500 ----------------------\n",
      "Minibatch loss: 0.577037\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 172 of 500 ----------------------\n",
      "Minibatch loss: 0.586763\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 173 of 500 ----------------------\n",
      "Minibatch loss: 0.513922\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 174 of 500 ----------------------\n",
      "Minibatch loss: 0.505405\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 175 of 500 ----------------------\n",
      "Minibatch loss: 0.560553\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 176 of 500 ----------------------\n",
      "Minibatch loss: 0.523239\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 177 of 500 ----------------------\n",
      "Minibatch loss: 0.572632\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 178 of 500 ----------------------\n",
      "Minibatch loss: 0.533271\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 179 of 500 ----------------------\n",
      "Minibatch loss: 0.569451\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 180 of 500 ----------------------\n",
      "Minibatch loss: 0.554982\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 181 of 500 ----------------------\n",
      "Minibatch loss: 0.618910\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 182 of 500 ----------------------\n",
      "Minibatch loss: 0.563693\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 183 of 500 ----------------------\n",
      "Minibatch loss: 0.537331\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 184 of 500 ----------------------\n",
      "Minibatch loss: 0.532834\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 185 of 500 ----------------------\n",
      "Minibatch loss: 0.578417\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 186 of 500 ----------------------\n",
      "Minibatch loss: 0.554954\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 187 of 500 ----------------------\n",
      "Minibatch loss: 0.561053\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 188 of 500 ----------------------\n",
      "Minibatch loss: 0.501225\n",
      "Minibatch train accuracy: 91.8%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 189 of 500 ----------------------\n",
      "Minibatch loss: 0.574931\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 190 of 500 ----------------------\n",
      "Minibatch loss: 0.576703\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 191 of 500 ----------------------\n",
      "Minibatch loss: 0.571982\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 192 of 500 ----------------------\n",
      "Minibatch loss: 0.573765\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 193 of 500 ----------------------\n",
      "Minibatch loss: 0.625546\n",
      "Minibatch train accuracy: 88.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 194 of 500 ----------------------\n",
      "Minibatch loss: 0.617724\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 195 of 500 ----------------------\n",
      "Minibatch loss: 0.565998\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 196 of 500 ----------------------\n",
      "Minibatch loss: 0.569762\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 197 of 500 ----------------------\n",
      "Minibatch loss: 0.558533\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 198 of 500 ----------------------\n",
      "Minibatch loss: 0.608170\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 199 of 500 ----------------------\n",
      "Minibatch loss: 0.506307\n",
      "Minibatch train accuracy: 91.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 200 of 500 ----------------------\n",
      "Minibatch loss: 0.564024\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 201 of 500 ----------------------\n",
      "Minibatch loss: 0.545830\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 202 of 500 ----------------------\n",
      "Minibatch loss: 0.593746\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 203 of 500 ----------------------\n",
      "Minibatch loss: 0.576794\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 204 of 500 ----------------------\n",
      "Minibatch loss: 0.579752\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 205 of 500 ----------------------\n",
      "Minibatch loss: 0.567916\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 206 of 500 ----------------------\n",
      "Minibatch loss: 0.572076\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 207 of 500 ----------------------\n",
      "Minibatch loss: 0.548495\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 208 of 500 ----------------------\n",
      "Minibatch loss: 0.531955\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 209 of 500 ----------------------\n",
      "Minibatch loss: 0.548360\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 210 of 500 ----------------------\n",
      "Minibatch loss: 0.536355\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 211 of 500 ----------------------\n",
      "Minibatch loss: 0.602661\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 212 of 500 ----------------------\n",
      "Minibatch loss: 0.563250\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 213 of 500 ----------------------\n",
      "Minibatch loss: 0.542754\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 214 of 500 ----------------------\n",
      "Minibatch loss: 0.533464\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 215 of 500 ----------------------\n",
      "Minibatch loss: 0.551146\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 216 of 500 ----------------------\n",
      "Minibatch loss: 0.525317\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 217 of 500 ----------------------\n",
      "Minibatch loss: 0.566896\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 218 of 500 ----------------------\n",
      "Minibatch loss: 0.520996\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 219 of 500 ----------------------\n",
      "Minibatch loss: 0.520773\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 220 of 500 ----------------------\n",
      "Minibatch loss: 0.530117\n",
      "Minibatch train accuracy: 92.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 221 of 500 ----------------------\n",
      "Minibatch loss: 0.543164\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 222 of 500 ----------------------\n",
      "Minibatch loss: 0.479832\n",
      "Minibatch train accuracy: 92.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 223 of 500 ----------------------\n",
      "Minibatch loss: 0.534253\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 224 of 500 ----------------------\n",
      "Minibatch loss: 0.619745\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 225 of 500 ----------------------\n",
      "Minibatch loss: 0.604933\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 226 of 500 ----------------------\n",
      "Minibatch loss: 0.572691\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 227 of 500 ----------------------\n",
      "Minibatch loss: 0.539991\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 228 of 500 ----------------------\n",
      "Minibatch loss: 0.512249\n",
      "Minibatch train accuracy: 92.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 229 of 500 ----------------------\n",
      "Minibatch loss: 0.595401\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 230 of 500 ----------------------\n",
      "Minibatch loss: 0.596678\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 231 of 500 ----------------------\n",
      "Minibatch loss: 0.570329\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 232 of 500 ----------------------\n",
      "Minibatch loss: 0.516957\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 233 of 500 ----------------------\n",
      "Minibatch loss: 0.567134\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 234 of 500 ----------------------\n",
      "Minibatch loss: 0.543182\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 235 of 500 ----------------------\n",
      "Minibatch loss: 0.536825\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 236 of 500 ----------------------\n",
      "Minibatch loss: 0.562355\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 237 of 500 ----------------------\n",
      "Minibatch loss: 0.623880\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 238 of 500 ----------------------\n",
      "Minibatch loss: 0.571403\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 239 of 500 ----------------------\n",
      "Minibatch loss: 0.622680\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 240 of 500 ----------------------\n",
      "Minibatch loss: 0.639167\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 241 of 500 ----------------------\n",
      "Minibatch loss: 0.599426\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 242 of 500 ----------------------\n",
      "Minibatch loss: 0.627403\n",
      "Minibatch train accuracy: 87.8%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 243 of 500 ----------------------\n",
      "Minibatch loss: 0.591689\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 244 of 500 ----------------------\n",
      "Minibatch loss: 0.557743\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 245 of 500 ----------------------\n",
      "Minibatch loss: 0.587889\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 246 of 500 ----------------------\n",
      "Minibatch loss: 0.573894\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 247 of 500 ----------------------\n",
      "Minibatch loss: 0.572785\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 248 of 500 ----------------------\n",
      "Minibatch loss: 0.551966\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 249 of 500 ----------------------\n",
      "Minibatch loss: 0.533716\n",
      "Minibatch train accuracy: 91.6%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 250 of 500 ----------------------\n",
      "Minibatch loss: 0.574944\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 251 of 500 ----------------------\n",
      "Minibatch loss: 0.534744\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 252 of 500 ----------------------\n",
      "Minibatch loss: 0.552514\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 253 of 500 ----------------------\n",
      "Minibatch loss: 0.553447\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 254 of 500 ----------------------\n",
      "Minibatch loss: 0.549747\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 255 of 500 ----------------------\n",
      "Minibatch loss: 0.534657\n",
      "Minibatch train accuracy: 91.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 256 of 500 ----------------------\n",
      "Minibatch loss: 0.523085\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 257 of 500 ----------------------\n",
      "Minibatch loss: 0.561213\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 258 of 500 ----------------------\n",
      "Minibatch loss: 0.554275\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 259 of 500 ----------------------\n",
      "Minibatch loss: 0.599759\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 260 of 500 ----------------------\n",
      "Minibatch loss: 0.634636\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 261 of 500 ----------------------\n",
      "Minibatch loss: 0.520347\n",
      "Minibatch train accuracy: 91.8%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 262 of 500 ----------------------\n",
      "Minibatch loss: 0.554484\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 263 of 500 ----------------------\n",
      "Minibatch loss: 0.584448\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 264 of 500 ----------------------\n",
      "Minibatch loss: 0.559757\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 265 of 500 ----------------------\n",
      "Minibatch loss: 0.571936\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 266 of 500 ----------------------\n",
      "Minibatch loss: 0.565040\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 267 of 500 ----------------------\n",
      "Minibatch loss: 0.625762\n",
      "Minibatch train accuracy: 88.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 268 of 500 ----------------------\n",
      "Minibatch loss: 0.612911\n",
      "Minibatch train accuracy: 88.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 269 of 500 ----------------------\n",
      "Minibatch loss: 0.527459\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 270 of 500 ----------------------\n",
      "Minibatch loss: 0.518413\n",
      "Minibatch train accuracy: 91.6%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 271 of 500 ----------------------\n",
      "Minibatch loss: 0.501635\n",
      "Minibatch train accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 272 of 500 ----------------------\n",
      "Minibatch loss: 0.475367\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 273 of 500 ----------------------\n",
      "Minibatch loss: 0.516744\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 274 of 500 ----------------------\n",
      "Minibatch loss: 0.506749\n",
      "Minibatch train accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 275 of 500 ----------------------\n",
      "Minibatch loss: 0.539265\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 276 of 500 ----------------------\n",
      "Minibatch loss: 0.602340\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 277 of 500 ----------------------\n",
      "Minibatch loss: 0.546212\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 278 of 500 ----------------------\n",
      "Minibatch loss: 0.582957\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 279 of 500 ----------------------\n",
      "Minibatch loss: 0.561849\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 280 of 500 ----------------------\n",
      "Minibatch loss: 0.515575\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 281 of 500 ----------------------\n",
      "Minibatch loss: 0.587009\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 282 of 500 ----------------------\n",
      "Minibatch loss: 0.596893\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 283 of 500 ----------------------\n",
      "Minibatch loss: 0.588297\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 284 of 500 ----------------------\n",
      "Minibatch loss: 0.627577\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 285 of 500 ----------------------\n",
      "Minibatch loss: 0.546507\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 286 of 500 ----------------------\n",
      "Minibatch loss: 0.581342\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 287 of 500 ----------------------\n",
      "Minibatch loss: 0.551089\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 288 of 500 ----------------------\n",
      "Minibatch loss: 0.560439\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 289 of 500 ----------------------\n",
      "Minibatch loss: 0.568033\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 290 of 500 ----------------------\n",
      "Minibatch loss: 0.608315\n",
      "Minibatch train accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 291 of 500 ----------------------\n",
      "Minibatch loss: 0.557171\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 292 of 500 ----------------------\n",
      "Minibatch loss: 0.531024\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 293 of 500 ----------------------\n",
      "Minibatch loss: 0.551483\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 294 of 500 ----------------------\n",
      "Minibatch loss: 0.604294\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 295 of 500 ----------------------\n",
      "Minibatch loss: 0.569738\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 296 of 500 ----------------------\n",
      "Minibatch loss: 0.521958\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 297 of 500 ----------------------\n",
      "Minibatch loss: 0.576257\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 298 of 500 ----------------------\n",
      "Minibatch loss: 0.583830\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 299 of 500 ----------------------\n",
      "Minibatch loss: 0.566498\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 300 of 500 ----------------------\n",
      "Minibatch loss: 0.537123\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 301 of 500 ----------------------\n",
      "Minibatch loss: 0.550196\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 302 of 500 ----------------------\n",
      "Minibatch loss: 0.542407\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 303 of 500 ----------------------\n",
      "Minibatch loss: 0.584691\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 304 of 500 ----------------------\n",
      "Minibatch loss: 0.544922\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 305 of 500 ----------------------\n",
      "Minibatch loss: 0.566872\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 306 of 500 ----------------------\n",
      "Minibatch loss: 0.587321\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 307 of 500 ----------------------\n",
      "Minibatch loss: 0.566349\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 308 of 500 ----------------------\n",
      "Minibatch loss: 0.572183\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 309 of 500 ----------------------\n",
      "Minibatch loss: 0.542880\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 310 of 500 ----------------------\n",
      "Minibatch loss: 0.511560\n",
      "Minibatch train accuracy: 91.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 311 of 500 ----------------------\n",
      "Minibatch loss: 0.521479\n",
      "Minibatch train accuracy: 92.1%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 312 of 500 ----------------------\n",
      "Minibatch loss: 0.604121\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 313 of 500 ----------------------\n",
      "Minibatch loss: 0.542176\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 314 of 500 ----------------------\n",
      "Minibatch loss: 0.576959\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 315 of 500 ----------------------\n",
      "Minibatch loss: 0.539933\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 316 of 500 ----------------------\n",
      "Minibatch loss: 0.597251\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 317 of 500 ----------------------\n",
      "Minibatch loss: 0.568718\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 318 of 500 ----------------------\n",
      "Minibatch loss: 0.585302\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 319 of 500 ----------------------\n",
      "Minibatch loss: 0.569827\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 320 of 500 ----------------------\n",
      "Minibatch loss: 0.586960\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 321 of 500 ----------------------\n",
      "Minibatch loss: 0.466798\n",
      "Minibatch train accuracy: 92.6%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 322 of 500 ----------------------\n",
      "Minibatch loss: 0.530781\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 323 of 500 ----------------------\n",
      "Minibatch loss: 0.540277\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.5%\n",
      "------------------- Epoch 324 of 500 ----------------------\n",
      "Minibatch loss: 0.589993\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 325 of 500 ----------------------\n",
      "Minibatch loss: 0.598677\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 326 of 500 ----------------------\n",
      "Minibatch loss: 0.541585\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 327 of 500 ----------------------\n",
      "Minibatch loss: 0.538494\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 328 of 500 ----------------------\n",
      "Minibatch loss: 0.550037\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 329 of 500 ----------------------\n",
      "Minibatch loss: 0.557422\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 330 of 500 ----------------------\n",
      "Minibatch loss: 0.574920\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 331 of 500 ----------------------\n",
      "Minibatch loss: 0.526472\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 332 of 500 ----------------------\n",
      "Minibatch loss: 0.513566\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 333 of 500 ----------------------\n",
      "Minibatch loss: 0.520701\n",
      "Minibatch train accuracy: 92.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 334 of 500 ----------------------\n",
      "Minibatch loss: 0.516416\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 335 of 500 ----------------------\n",
      "Minibatch loss: 0.550860\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 336 of 500 ----------------------\n",
      "Minibatch loss: 0.540155\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 337 of 500 ----------------------\n",
      "Minibatch loss: 0.505661\n",
      "Minibatch train accuracy: 93.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 338 of 500 ----------------------\n",
      "Minibatch loss: 0.533931\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 339 of 500 ----------------------\n",
      "Minibatch loss: 0.555503\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 340 of 500 ----------------------\n",
      "Minibatch loss: 0.586910\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 341 of 500 ----------------------\n",
      "Minibatch loss: 0.578310\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 342 of 500 ----------------------\n",
      "Minibatch loss: 0.556945\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 343 of 500 ----------------------\n",
      "Minibatch loss: 0.596998\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 344 of 500 ----------------------\n",
      "Minibatch loss: 0.571970\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 345 of 500 ----------------------\n",
      "Minibatch loss: 0.620809\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 346 of 500 ----------------------\n",
      "Minibatch loss: 0.617957\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 347 of 500 ----------------------\n",
      "Minibatch loss: 0.539423\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 348 of 500 ----------------------\n",
      "Minibatch loss: 0.520992\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 349 of 500 ----------------------\n",
      "Minibatch loss: 0.583599\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 350 of 500 ----------------------\n",
      "Minibatch loss: 0.597175\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 351 of 500 ----------------------\n",
      "Minibatch loss: 0.556497\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 352 of 500 ----------------------\n",
      "Minibatch loss: 0.589875\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "------------------- Epoch 353 of 500 ----------------------\n",
      "Minibatch loss: 0.519910\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 354 of 500 ----------------------\n",
      "Minibatch loss: 0.584958\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 355 of 500 ----------------------\n",
      "Minibatch loss: 0.588543\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 356 of 500 ----------------------\n",
      "Minibatch loss: 0.585485\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 357 of 500 ----------------------\n",
      "Minibatch loss: 0.561354\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 358 of 500 ----------------------\n",
      "Minibatch loss: 0.500529\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 359 of 500 ----------------------\n",
      "Minibatch loss: 0.556959\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 360 of 500 ----------------------\n",
      "Minibatch loss: 0.612509\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 361 of 500 ----------------------\n",
      "Minibatch loss: 0.556787\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 362 of 500 ----------------------\n",
      "Minibatch loss: 0.573912\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 363 of 500 ----------------------\n",
      "Minibatch loss: 0.525000\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 364 of 500 ----------------------\n",
      "Minibatch loss: 0.576314\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 365 of 500 ----------------------\n",
      "Minibatch loss: 0.588517\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 366 of 500 ----------------------\n",
      "Minibatch loss: 0.590941\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 367 of 500 ----------------------\n",
      "Minibatch loss: 0.549780\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 368 of 500 ----------------------\n",
      "Minibatch loss: 0.581216\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 369 of 500 ----------------------\n",
      "Minibatch loss: 0.550075\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 370 of 500 ----------------------\n",
      "Minibatch loss: 0.562198\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 371 of 500 ----------------------\n",
      "Minibatch loss: 0.549872\n",
      "Minibatch train accuracy: 92.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 372 of 500 ----------------------\n",
      "Minibatch loss: 0.559351\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 373 of 500 ----------------------\n",
      "Minibatch loss: 0.571802\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "------------------- Epoch 374 of 500 ----------------------\n",
      "Minibatch loss: 0.548868\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 375 of 500 ----------------------\n",
      "Minibatch loss: 0.579235\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 376 of 500 ----------------------\n",
      "Minibatch loss: 0.608999\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 377 of 500 ----------------------\n",
      "Minibatch loss: 0.554539\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 378 of 500 ----------------------\n",
      "Minibatch loss: 0.603105\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 379 of 500 ----------------------\n",
      "Minibatch loss: 0.628535\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 380 of 500 ----------------------\n",
      "Minibatch loss: 0.581700\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 381 of 500 ----------------------\n",
      "Minibatch loss: 0.642577\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 382 of 500 ----------------------\n",
      "Minibatch loss: 0.556873\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 383 of 500 ----------------------\n",
      "Minibatch loss: 0.493451\n",
      "Minibatch train accuracy: 92.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 384 of 500 ----------------------\n",
      "Minibatch loss: 0.538378\n",
      "Minibatch train accuracy: 92.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 385 of 500 ----------------------\n",
      "Minibatch loss: 0.545358\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 386 of 500 ----------------------\n",
      "Minibatch loss: 0.532847\n",
      "Minibatch train accuracy: 91.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 387 of 500 ----------------------\n",
      "Minibatch loss: 0.520898\n",
      "Minibatch train accuracy: 92.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 388 of 500 ----------------------\n",
      "Minibatch loss: 0.558993\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 389 of 500 ----------------------\n",
      "Minibatch loss: 0.554189\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 390 of 500 ----------------------\n",
      "Minibatch loss: 0.555272\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 391 of 500 ----------------------\n",
      "Minibatch loss: 0.587415\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 392 of 500 ----------------------\n",
      "Minibatch loss: 0.577981\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 393 of 500 ----------------------\n",
      "Minibatch loss: 0.601984\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 394 of 500 ----------------------\n",
      "Minibatch loss: 0.616781\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.7%\n",
      "------------------- Epoch 395 of 500 ----------------------\n",
      "Minibatch loss: 0.561164\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 396 of 500 ----------------------\n",
      "Minibatch loss: 0.613678\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 397 of 500 ----------------------\n",
      "Minibatch loss: 0.559321\n",
      "Minibatch train accuracy: 89.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 398 of 500 ----------------------\n",
      "Minibatch loss: 0.594490\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 399 of 500 ----------------------\n",
      "Minibatch loss: 0.524569\n",
      "Minibatch train accuracy: 92.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 400 of 500 ----------------------\n",
      "Minibatch loss: 0.511440\n",
      "Minibatch train accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 401 of 500 ----------------------\n",
      "Minibatch loss: 0.585196\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 402 of 500 ----------------------\n",
      "Minibatch loss: 0.538554\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 403 of 500 ----------------------\n",
      "Minibatch loss: 0.538489\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 404 of 500 ----------------------\n",
      "Minibatch loss: 0.565520\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 405 of 500 ----------------------\n",
      "Minibatch loss: 0.550203\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 406 of 500 ----------------------\n",
      "Minibatch loss: 0.535439\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 407 of 500 ----------------------\n",
      "Minibatch loss: 0.553988\n",
      "Minibatch train accuracy: 90.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 408 of 500 ----------------------\n",
      "Minibatch loss: 0.533972\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 409 of 500 ----------------------\n",
      "Minibatch loss: 0.555852\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 410 of 500 ----------------------\n",
      "Minibatch loss: 0.522664\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 411 of 500 ----------------------\n",
      "Minibatch loss: 0.585350\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 412 of 500 ----------------------\n",
      "Minibatch loss: 0.573902\n",
      "Minibatch train accuracy: 88.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 413 of 500 ----------------------\n",
      "Minibatch loss: 0.578651\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 414 of 500 ----------------------\n",
      "Minibatch loss: 0.549357\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 415 of 500 ----------------------\n",
      "Minibatch loss: 0.569400\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 88.6%\n",
      "------------------- Epoch 416 of 500 ----------------------\n",
      "Minibatch loss: 0.557466\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 417 of 500 ----------------------\n",
      "Minibatch loss: 0.504634\n",
      "Minibatch train accuracy: 91.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 418 of 500 ----------------------\n",
      "Minibatch loss: 0.616770\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 419 of 500 ----------------------\n",
      "Minibatch loss: 0.609692\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 420 of 500 ----------------------\n",
      "Minibatch loss: 0.555348\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 421 of 500 ----------------------\n",
      "Minibatch loss: 0.566710\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 422 of 500 ----------------------\n",
      "Minibatch loss: 0.561026\n",
      "Minibatch train accuracy: 89.8%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 423 of 500 ----------------------\n",
      "Minibatch loss: 0.541187\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 424 of 500 ----------------------\n",
      "Minibatch loss: 0.521192\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 425 of 500 ----------------------\n",
      "Minibatch loss: 0.541682\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 426 of 500 ----------------------\n",
      "Minibatch loss: 0.553163\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 427 of 500 ----------------------\n",
      "Minibatch loss: 0.561517\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 428 of 500 ----------------------\n",
      "Minibatch loss: 0.622053\n",
      "Minibatch train accuracy: 88.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 429 of 500 ----------------------\n",
      "Minibatch loss: 0.580326\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 430 of 500 ----------------------\n",
      "Minibatch loss: 0.574228\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 431 of 500 ----------------------\n",
      "Minibatch loss: 0.542234\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 432 of 500 ----------------------\n",
      "Minibatch loss: 0.546142\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 433 of 500 ----------------------\n",
      "Minibatch loss: 0.566939\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 434 of 500 ----------------------\n",
      "Minibatch loss: 0.532359\n",
      "Minibatch train accuracy: 91.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 435 of 500 ----------------------\n",
      "Minibatch loss: 0.523928\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 436 of 500 ----------------------\n",
      "Minibatch loss: 0.529911\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 437 of 500 ----------------------\n",
      "Minibatch loss: 0.587016\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 438 of 500 ----------------------\n",
      "Minibatch loss: 0.654097\n",
      "Minibatch train accuracy: 89.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 439 of 500 ----------------------\n",
      "Minibatch loss: 0.558860\n",
      "Minibatch train accuracy: 90.3%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 440 of 500 ----------------------\n",
      "Minibatch loss: 0.592695\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 441 of 500 ----------------------\n",
      "Minibatch loss: 0.597589\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 442 of 500 ----------------------\n",
      "Minibatch loss: 0.531121\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 443 of 500 ----------------------\n",
      "Minibatch loss: 0.531517\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 444 of 500 ----------------------\n",
      "Minibatch loss: 0.530118\n",
      "Minibatch train accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 445 of 500 ----------------------\n",
      "Minibatch loss: 0.507575\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 446 of 500 ----------------------\n",
      "Minibatch loss: 0.595446\n",
      "Minibatch train accuracy: 88.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 447 of 500 ----------------------\n",
      "Minibatch loss: 0.630130\n",
      "Minibatch train accuracy: 88.4%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 448 of 500 ----------------------\n",
      "Minibatch loss: 0.567999\n",
      "Minibatch train accuracy: 89.7%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 449 of 500 ----------------------\n",
      "Minibatch loss: 0.563260\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 450 of 500 ----------------------\n",
      "Minibatch loss: 0.591690\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 451 of 500 ----------------------\n",
      "Minibatch loss: 0.601748\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 452 of 500 ----------------------\n",
      "Minibatch loss: 0.633985\n",
      "Minibatch train accuracy: 88.8%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 453 of 500 ----------------------\n",
      "Minibatch loss: 0.577260\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 454 of 500 ----------------------\n",
      "Minibatch loss: 0.602434\n",
      "Minibatch train accuracy: 89.3%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 455 of 500 ----------------------\n",
      "Minibatch loss: 0.574766\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 89.4%\n",
      "------------------- Epoch 456 of 500 ----------------------\n",
      "Minibatch loss: 0.510277\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 457 of 500 ----------------------\n",
      "Minibatch loss: 0.534775\n",
      "Minibatch train accuracy: 91.5%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 458 of 500 ----------------------\n",
      "Minibatch loss: 0.573786\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 459 of 500 ----------------------\n",
      "Minibatch loss: 0.539318\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 460 of 500 ----------------------\n",
      "Minibatch loss: 0.574527\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 461 of 500 ----------------------\n",
      "Minibatch loss: 0.527895\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 462 of 500 ----------------------\n",
      "Minibatch loss: 0.532326\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 463 of 500 ----------------------\n",
      "Minibatch loss: 0.540980\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 464 of 500 ----------------------\n",
      "Minibatch loss: 0.599164\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 465 of 500 ----------------------\n",
      "Minibatch loss: 0.523210\n",
      "Minibatch train accuracy: 91.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 466 of 500 ----------------------\n",
      "Minibatch loss: 0.523723\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 467 of 500 ----------------------\n",
      "Minibatch loss: 0.541131\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 468 of 500 ----------------------\n",
      "Minibatch loss: 0.523924\n",
      "Minibatch train accuracy: 92.1%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 469 of 500 ----------------------\n",
      "Minibatch loss: 0.522192\n",
      "Minibatch train accuracy: 91.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 470 of 500 ----------------------\n",
      "Minibatch loss: 0.542330\n",
      "Minibatch train accuracy: 90.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 471 of 500 ----------------------\n",
      "Minibatch loss: 0.559165\n",
      "Minibatch train accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 472 of 500 ----------------------\n",
      "Minibatch loss: 0.597236\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 473 of 500 ----------------------\n",
      "Minibatch loss: 0.591902\n",
      "Minibatch train accuracy: 89.0%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 474 of 500 ----------------------\n",
      "Minibatch loss: 0.598795\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 475 of 500 ----------------------\n",
      "Minibatch loss: 0.571117\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 476 of 500 ----------------------\n",
      "Minibatch loss: 0.620216\n",
      "Minibatch train accuracy: 88.7%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 477 of 500 ----------------------\n",
      "Minibatch loss: 0.617790\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 478 of 500 ----------------------\n",
      "Minibatch loss: 0.556822\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 479 of 500 ----------------------\n",
      "Minibatch loss: 0.554576\n",
      "Minibatch train accuracy: 90.5%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 480 of 500 ----------------------\n",
      "Minibatch loss: 0.590185\n",
      "Minibatch train accuracy: 88.9%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 481 of 500 ----------------------\n",
      "Minibatch loss: 0.575612\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 482 of 500 ----------------------\n",
      "Minibatch loss: 0.533266\n",
      "Minibatch train accuracy: 92.3%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 483 of 500 ----------------------\n",
      "Minibatch loss: 0.586310\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 484 of 500 ----------------------\n",
      "Minibatch loss: 0.566020\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 485 of 500 ----------------------\n",
      "Minibatch loss: 0.580804\n",
      "Minibatch train accuracy: 89.4%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 486 of 500 ----------------------\n",
      "Minibatch loss: 0.553955\n",
      "Minibatch train accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 487 of 500 ----------------------\n",
      "Minibatch loss: 0.570310\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 88.9%\n",
      "------------------- Epoch 488 of 500 ----------------------\n",
      "Minibatch loss: 0.562566\n",
      "Minibatch train accuracy: 89.6%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 489 of 500 ----------------------\n",
      "Minibatch loss: 0.567212\n",
      "Minibatch train accuracy: 90.0%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 490 of 500 ----------------------\n",
      "Minibatch loss: 0.560104\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.0%\n",
      "------------------- Epoch 491 of 500 ----------------------\n",
      "Minibatch loss: 0.534019\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 88.8%\n",
      "------------------- Epoch 492 of 500 ----------------------\n",
      "Minibatch loss: 0.579703\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 493 of 500 ----------------------\n",
      "Minibatch loss: 0.541680\n",
      "Minibatch train accuracy: 91.6%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 494 of 500 ----------------------\n",
      "Minibatch loss: 0.589451\n",
      "Minibatch train accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 495 of 500 ----------------------\n",
      "Minibatch loss: 0.544342\n",
      "Minibatch train accuracy: 91.0%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 496 of 500 ----------------------\n",
      "Minibatch loss: 0.551472\n",
      "Minibatch train accuracy: 91.7%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 497 of 500 ----------------------\n",
      "Minibatch loss: 0.561525\n",
      "Minibatch train accuracy: 90.1%\n",
      "Validation accuracy: 89.1%\n",
      "------------------- Epoch 498 of 500 ----------------------\n",
      "Minibatch loss: 0.521508\n",
      "Minibatch train accuracy: 90.7%\n",
      "Validation accuracy: 89.2%\n",
      "------------------- Epoch 499 of 500 ----------------------\n",
      "Minibatch loss: 0.535165\n",
      "Minibatch train accuracy: 90.2%\n",
      "Validation accuracy: 89.3%\n",
      "------------------- Epoch 500 of 500 ----------------------\n",
      "Minibatch loss: 0.518451\n",
      "Minibatch train accuracy: 90.8%\n",
      "Validation accuracy: 89.1%\n",
      "*************** Finished ***************\n",
      "Best Valid Accuracy: 89.5% on L2: 1.000000e-03, WeightScale: 1.000000e-01 -----------------\n",
      "*************** Test accuracy: 94.9% *****************\n"
     ]
    }
   ],
   "source": [
    "hidden_dims = [1024, 300, 50]\n",
    "batches_num = 100\n",
    "batch_size = 1024#train_dataset.shape[0] / batches_num\n",
    "\n",
    "num_epochs = 500\n",
    "dropout_prob = 0.5\n",
    "\n",
    "l2 = 1e-3\n",
    "lr_start = 1e-3\n",
    "lr_decay_steps = 100000\n",
    "lr_decay_rate = 0.96\n",
    "ws = 0.1\n",
    "\n",
    "print ('-----------------------------------------')\n",
    "print ('Start NN l2 %e weight_scale %e ' % (l2, ws))\n",
    "\n",
    "mln = MultiLayerNet(input_dim, hidden_dims, num_classes, l2)\n",
    "solver = Solver(mln, train_dataset, train_labels, valid_dataset, valid_labels, \n",
    "                batch_size, num_epochs, dropout_prob, \n",
    "                lr_start, lr_decay_steps, lr_decay_rate)\n",
    "(valid_loss, valid_accuracy, valid_weights, valid_biases) = solver.train()\n",
    "\n",
    "print(\"*************** Finished ***************\")\n",
    "print(\"Best Valid Accuracy: %.1f%% on L2: %e, WeightScale: %e -----------------\" % (valid_accuracy*100, l2, ws))\n",
    "\n",
    "test_accuracy_model = MultiLayerNet(input_dim, hidden_dims, num_classes, l2, ws, valid_weights, valid_biases)\n",
    "with tf.Session(graph=test_accuracy_model.graph) as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    test_accuracy = test_accuracy_model.accuracy.eval(feed_dict={test_accuracy_model.X:test_dataset, \n",
    "                                                             test_accuracy_model.Y:test_labels, \n",
    "                                                             test_accuracy_model.dropout: 1.0})\n",
    "print(\"*************** Test accuracy: %.1f%% *****************\" % (test_accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
